library(leaflet)
library(ggvis)
library(shinydashboard)
library(DT) #for datatables
library(shinyjs) #for reset button
library(mapview)
library(leaflet)
library(ggvis)
library(shinydashboard)
library(DT) #for datatables
library(shinyjs) #for reset button
library(mapview)
shiny::runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
## Create reactive functions for both services received and districts table ##
library(shiny)
library(tidyr)
library(dplyr)
library(ggplot2)
library(scales)
library(grid)
library(maps)
library(ggmap)
library(reshape)
library(leaflet)
library(ggvis)
library(DT)
library(shinydashboard)
library(extrafontdb)
library(extrafont)
library(leaflet)
library(ggvis)
library(shinydashboard)
library(DT) #for datatables
library(shinyjs) #for reset button
library(mapview)
sketch = htmltools::withTags(table(
class = 'display',
thead(
tr(
th(rowspan = 2, 'Species'),
th(colspan = 2, 'Sepal'),
th(colspan = 2, 'Petal')
),
tr(
lapply(rep(c('Length', 'Width'), 2), th)
)
)
))
print(sketch)
lapply(rep(c('Length', 'Width'), 2), th)
tr(
lapply(rep(c('Length', 'Width'), 2), th)
)
sketch = htmltools::withTags(table(
class = 'display',
thead(
tr(
th(rowspan = 2, 'Postal Code'),
th(rowspan = 2, 'Locale'),
th(colspan = 2, 'Clean'),
th(colspan = 2, 'All')
),
tr(
lapply(rep(c('Length', 'Width'), 2), th)
)
)
))
print(sketch)
shiny::runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
options(shiny.reactlog=TRUE)
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
install.packages("googleVis")
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
which R
install.packages("rJava")
library(rJava)
dta <- read.csv("~/Downloads/final_campus_groupings_2016-08-31_school_level.csv", as.is=T, header=T)
nces.raw <- read.csv("~/Downloads/sc131a.csv", as.is=T, header=T)
dta.campus <- read.csv("~/Downloads/final_campus_groupings_2016-08-31.csv", as.is=T, header=T)
nces.raw$combined.addr <- paste(nces.raw$LSTREE, nces.raw$LCITY, nces.raw$LSTATE, nces.raw$LZIP, sep=' ')
View(dta)
View(nces.raw)
real.schools <- read.csv("~/Downloads/fy2016_schools_demog_2016-08-30.csv", as.is=T, header=T)
View(real.schools)
names(real.schools)
real.schools$combined.addr <- paste(real.schools$address, real.schools$city, real.schools$postal_cd, real.schools$zip, sep=' ')
View(real.schools)
dta <- merge(dta, real.schools[,c("school_esh_id", "combined.addr")], by="school_esh_id")
View(dta)
View(dta.campus)
dta.agg <- aggregate(dta$combined.addr, by=list(dta$campus_id), FUN=length(unique(x)))
dta.agg <- aggregate(dta$combined.addr, by=list(dta$campus_id), FUN=function(x){y <- length(unique(x)); return(y)})
## aggregate number of unique addresses by campus id
dta.agg <- aggregate(dta$combined.addr, by=list(dta$campus_id),
FUN=function(x){length(unique(x))})
dta <- read.csv("~/Downloads/final_campus_groupings_2016-08-31_school_level.csv", as.is=T, header=T)
dta.campus <- read.csv("~/Downloads/final_campus_groupings_2016-08-31.csv", as.is=T, header=T)
#nces.raw <- read.csv("~/Downloads/sc131a.csv", as.is=T, header=T)
real.schools <- read.csv("~/Downloads/fy2016_schools_demog_2016-08-30.csv", as.is=T, header=T)
real.schools$combined.addr <- paste(real.schools$address, real.schools$city, real.schools$postal_cd, real.schools$zip, sep=' ')
## merge in address info to campus
dta <- merge(dta, real.schools[,c("school_esh_id", "combined.addr")], by="school_esh_id")
## aggregate number of unique addresses by campus id
#dta.agg <- aggregate(dta$combined.addr, by=list(dta$campus_id),
#                     FUN=function(x){length(unique(x))})
dta.store <- as.data.frame(campus_id == unique(dta$campus_id))
dta.store <- as.data.frame(campus_id = unique(dta$campus_id))
View(dta)
length(unique(dta$campus_id))
dta.store <- as.data.frame(campus_id = unique(dta$campus_id))
dta.store <- data.frame(campus_id = unique(dta$campus_id))
View(dta)
View(dta.campus)
dta <- merge(dta, dta.campus[,c("campus.id", "num.schools")], by.x="campus_id", by.y="campus.id", all.x=T)
View(dta)
table(dta$num.schools)
View(dta.campus)
dta.campus$campus.id <- paste("campus_", dta.campus$campus.id, sep='')
dta$num.schools <- NULL
dta <- merge(dta, dta.campus[,c("campus.id", "num.schools")], by.x="campus_id", by.y="campus.id", all.x=T)
View(dta)
dta <- dta[dta$num.schools > 1,]
View(dta)
dta.store <- data.frame(campus_id = unique(dta$campus_id))
View(dta.store)
for (i in 1:nrow(dta.store$campus_id)){
dta.sub <- dta[dta$campus_id == dta.store$campus_id[i],]
dta.store$unq.add[i] <- length(unique(dta.sub$combined.addr))
}
nrow(dta.store$campus_id)
View(dta.store)
nrow(dta.store)
for (i in 1:nrow(dta.store)){
dta.sub <- dta[dta$campus_id == dta.store$campus_id[i],]
dta.store$unq.add[i] <- length(unique(dta.sub$combined.addr))
}
View(dta.store)
table(dta.store$unq.add)
nrow(dta.store[dta.store$unq.add > 1,])
nrow(dta.store[dta.store$unq.add > 1,]) / nrow(dta.store)
sample.camp.ids <- sample(dta.store$campus_id[dta.store$unq.add > 1], 10, replace=F)
sub <- dta[dta$campus_id == sample.camp.ids[2],]
View(sub)
sub <- dta[dta$campus_id == sample.camp.ids[3],]
View(sub)
sub <- dta[dta$campus_id == sample.camp.ids[4],]
View(sub)
shiny::runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
shiny::runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
shiny::runApp('Desktop/ficher/Shiny')
a <- c(0,1,2,3,4,5)
combn(a,2)
x <- 54
y <- 55
x=y
if (x=y)
ifelse(x=y)
x
y
which(x=y)
if (x==y)
^c
x==y
x <- 40
y <- 40
x <= y
as.factor(x)
factor(x)
is.factor(x)
dta <- read.csv("~/Downloads/470_status.csv", as.is=T, header=T, stringsAsFactors = F)
View(dta)
sub <- dta[dta$esh_id == 881807,]
View(sub)
## Clearing memory
rm(list=ls())
sys.calls()[[1]] [[2]]
wd <- setwd(".")
setwd(wd)
dta <- read.csv("~/Downloads/Chase5337_Activity_20170217.CSV", as.is=T, header=T, stringsAsFactors=F)
View(dta)
dta.sub <- dta[dta$Amount < 0,]
View(dta.sub)
sum(dta.sub$Amount)
4000 - 2146.21
sum(dta$Amount)
dta <- read.csv("~/Downloads/Chase5337_Activity_20170227.CSV", as.is=T, header=T)
View(dta)
dta.sub <- dta[dta$Amount < 0,]
sum(dta.sub$Amount)
## Clearing memory
rm(list=ls())
setwd("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/Org-Wide Projects/Clustering Mega and Large Districts/code/")
## load packages
## force rJava to load on Mac 10 El Capitan
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
options(java.parameters = "-Xmx4g" )
library(rJava)
library(RJDBC)
library(ggplot2)
library(gridExtra) ## for grid.arrange function
library(fpc) ## for pamk function
library(cluster) ## for pam and clusGap function
## source function
source("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/General Resources/R_database_access/correct_dataset.R")
source("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/General Resources/R_database_access/db_credentials.R")
##**************************************************************************************************************************************************
## QUERY THE DB -- SQL
## load PostgreSQL Driver
pgsql <- JDBC("org.postgresql.Driver", "~/Downloads/postgresql-9.4.1209.jar", "`")
## connect to the database
con <- dbConnect(pgsql, url=url, user=user, password=password)
## query function
querydb <- function(query_name){
query <- readChar(query_name, file.info(query_name)$size)
data <- dbGetQuery(con, query)
return(data)
}
dd.2016 <- querydb("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/General Resources/R_database_access/SQL Scripts/2016_deluxe_districts_crusher_materialized.SQL")
dd.2016 <- correct.dataset(dd.2016, sots.flag=0, services.flag=0)
dta.470 <- querydb("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/General Resources/R_database_access/SQL Scripts/Form470s.SQL")
bens <- querydb("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/General Resources/R_database_access/SQL Scripts/Entity_Bens.SQL")
## disconnect from database
dbDisconnect(con)
##**************************************************************************************************************************************************
## subset and format data
## 470 data formatting
## format the column names (take out capitalization and spaces)
names(dta.470) <- tolower(names(dta.470))
names(dta.470) <- gsub(" ", ".", names(dta.470))
## rename column "function"
names(dta.470)[names(dta.470) == 'function'] <- 'function1'
## rename column "470.number"
names(dta.470)[names(dta.470) == '470.number'] <- 'X470.number'
## merge in BENs to DD
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## keep only unique bens (since some districts file multiple Form 470s)
dta.470 <- dta.470[which(!duplicated(dta.470$ben)),]
## merge in form 470 info (to determine if they've filed one)
dd.2016 <- merge(dd.2016, dta.470[,c('ben', 'X470.number')], by='ben', all.x=T)
## create an indicator for whether a district has filed a form 470
dd.2016$form_470 <- ifelse(!is.na(dd.2016$X470.number), TRUE, FALSE)
## take out duplicated esh_ids
dd.2016 <- dd.2016[!duplicated(dd.2016$esh_id),]
## select mega and large districts
dta <- dd.2016
dta.mega.large <- dd.2016[which(dd.2016$district_size %in% c('Large', 'Mega')),]
pdf("../figures/visualize_raw.pdf", height=4, width=10)
plot1 <- ggplot(dta, aes(num_students, num_schools, color=district_size)) + geom_point()
plot2 <- ggplot(dta, aes(num_students, num_schools, color=district_size)) + xlim(0,225000) + ylim(0,400) + geom_point()
grid.arrange(plot1, plot2, ncol=2)
dev.off()
## K-Means:
## K-means clustering can handle larger datasets than hierarchical cluster approaches.
## Additionally, observations are not permanently committed to a cluster.
## They are moved when doing so improves the overall solution.
## However, the use of means implies that all variables must be continuous and the approach can be severely affected by outliers.
## They also perform poorly in the presence of non-convex (e.g., U-shaped) clusters.
set.seed(455)
dtaCluster <- kmeans(dta[,c('num_students', 'num_schools')], 20, nstart=20)
dta$cluster_id <- as.factor(dtaCluster$cluster)
dtaCluster <- kmeans(dta.mega.large[,c('num_students', 'num_schools')], 8, nstart=20)
dta.mega.large$cluster_id_mega_large <- as.factor(dtaCluster$cluster)
## plot clusters -- all
pdf("../figures/visualize_clusters.pdf", height=6, width=10)
plot1 <- ggplot(dta, aes(num_students, num_schools, color=cluster_id)) + geom_point()
plot2 <- ggplot(dta, aes(num_students, num_schools, color=cluster_id)) + xlim(0,225000) + ylim(0,400) + geom_point()
grid.arrange(plot1, plot2, ncol=2)
dev.off()
## plot clusters -- mega and large
pdf("../figures/visualize_clusters_mega_large.pdf", height=4, width=10)
plot1 <- ggplot(dta.mega.large, aes(num_students, num_schools, color=cluster_id_mega_large)) + geom_point()
plot2 <- ggplot(dta.mega.large, aes(num_students, num_schools, color=cluster_id_mega_large)) + xlim(0,225000) + ylim(0,400) + geom_point()
grid.arrange(plot1, plot2, ncol=2)
dev.off()
## merge in both indicators
dta <- merge(dta, dta.mega.large[,c('esh_id', 'cluster_id_mega_large')], by='esh_id', all.x=T)
dta$cluster_id <- as.numeric(as.character(dta$cluster_id))
dta$cluster_id_mega_large <- as.numeric(as.character(dta$cluster_id_mega_large))
pdf("../figures/distribution_of_cluster_id.pdf", height=5, width=5)
hist(dta$cluster_id, col=rgb(0,0,0,0.6), border=F, xlim=c(0,20),
main="All Districts", xlab="cluster id")
dev.off()
pdf("../figures/distribution_of_cluster_id_mega_large.pdf", height=5, width=5)
hist(dta$cluster_id_mega_large, col=rgb(0,0,0,0.6), border=F, xlim=c(1,8),
main="Mega and Large", xlab="cluster id")
dev.off()
names(dta)
dta.agg <- aggregate(dta[,c('num_students', 'num_schools')], by=list(dta$cluster_id), FUN=mean, na.rm=T)
View(dta.agg)
dta.mega.large.agg <- aggregate(dta.mega.large[,c('num_students', 'num_schools')], by=list(dta$cluster_id_mega_large), FUN=mean, na.rm=T)
dta.mega.large.agg <- aggregate(dta.mega.large[,c('num_students', 'num_schools')], by=list(dta.mega.large$cluster_id_mega_large), FUN=mean, na.rm=T)
View(dta.mega.large.agg)
write.csv(dta, "../data/clustered_all_districts.csv", row.names=F)
## subset to variables: num_students, num_schools, district_size, form_470, cluster_id
dta <- dta[,c('esh_id', 'form_470', 'cluster_id', 'cluster_id_mega_large')]
write.csv(dta, "../data/clustered_all_districts.csv", row.names=F)
write.csv(dta, "../data/clustered_all_districts.csv", row.names=F)
write.csv(dta.agg, "../data/aggregated_means_clusters_all_districts.csv", row.names=F)
write.csv(dta.mega.large.agg, "../data/aggregated_means_clusters_mega_and_large_districts.csv", row.names=F)
View(dta.agg)
View(dta.agg)
View(dta)
