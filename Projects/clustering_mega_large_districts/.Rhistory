## running locally or on the server:
local <- 1
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
wd <- setwd(".")
setwd(wd)
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## =====================================================================================================
##
## OKR #2: FORM 470
## DEFINE WHETHER A FORM 470 IS MEETING GOALS FOR BOTH IA AND WAN
## EVENTUALLY RUN ON THE AWS SERVER AND MERGED INTO THE DB AND SALESFORCE
##
## =====================================================================================================
## Clearing memory
rm(list=ls())
## running locally or on the server:
local <- 1
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(rJava)
library(RJDBC)
library(DBI)
library(dotenv)
options(java.parameters = "-Xmx4g" )
## retrieve date (in order to accurately timestamp files)
date <- Sys.time()
date <- gsub("PST", "", date)
date <- gsub(" ", "_", date)
date <- gsub(":", ".", date)
## source function
source("source_env")
source("source_env.R")
source_env("~/.env")
## Clearing memory
rm(list=ls())
## running locally or on the server:
local <- 1
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(rJava)
library(RJDBC)
library(DBI)
library(dotenv)
options(java.parameters = "-Xmx4g" )
## retrieve date (in order to accurately timestamp files)
date <- Sys.time()
date <- gsub("PST", "", date)
date <- gsub(" ", "_", date)
date <- gsub(":", ".", date)
## source function
source("source_env.R")
source_env("~/.env")
## Clearing memory
rm(list=ls())
## running locally or on the server:
local <- 1
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(rJava)
library(RJDBC)
library(DBI)
library(dotenv)
options(java.parameters = "-Xmx4g" )
## retrieve date (in order to accurately timestamp files)
date <- Sys.time()
date <- gsub("PST", "", date)
date <- gsub(" ", "_", date)
date <- gsub(":", ".", date)
## source function
source("source_env.R")
source_env("~/.env")
path.to.database.info <- paste(github_path, "General_Resources/common_functions/", sep="")
github_path
## =====================================================================================================
##
## OKR #2: FORM 470
## DEFINE WHETHER A FORM 470 IS MEETING GOALS FOR BOTH IA AND WAN
## EVENTUALLY RUN ON THE AWS SERVER AND MERGED INTO THE DB AND SALESFORCE
##
## =====================================================================================================
## Clearing memory
rm(list=ls())
## running locally or on the server:
local <- 1
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(rJava)
library(RJDBC)
library(DBI)
library(dotenv)
options(java.parameters = "-Xmx4g" )
## retrieve date (in order to accurately timestamp files)
date <- Sys.time()
date <- gsub("PST", "", date)
date <- gsub(" ", "_", date)
date <- gsub(":", ".", date)
## source function
source("source_env.R")
source_env("~/.env")
## =====================================================================================================
##
## OKR #2: FORM 470
## DEFINE WHETHER A FORM 470 IS MEETING GOALS FOR BOTH IA AND WAN
## EVENTUALLY RUN ON THE AWS SERVER AND MERGED INTO THE DB AND SALESFORCE
##
## =====================================================================================================
## Clearing memory
rm(list=ls())
## running locally or on the server:
local <- 1
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(rJava)
library(RJDBC)
library(DBI)
library(dotenv)
options(java.parameters = "-Xmx4g" )
## retrieve date (in order to accurately timestamp files)
date <- Sys.time()
date <- gsub("PST", "", date)
date <- gsub(" ", "_", date)
date <- gsub(":", ".", date)
## source function
source("source_env.R")
source_env("~/.env")
#if (local == 1){
#  path.to.database.info <- paste(github_path, "General_Resources/common_functions/", sep="")
#} else{
#  path.to.database.info <- "../../R_database_access/"
#}
source(paste(github_path, "General_Resources/common_functions/correct_dataset.R", sep=""))
source(paste(github_path, "General_Resources/common_functions/db_credentials.R", sep=""))
pgsql <- JDBC("org.postgresql.Driver", paste(github_path, "General_Resources/R_database_access/postgresql-9.4.1212.jre7.jar", sep=""), "`")
con <- dbConnect(pgsql, url=url, user=user, password=password)
## query function
querydb <- function(query_name){
query <- readChar(query_name, file.info(query_name)$size)
data <- dbGetQuery(con, query)
return(data)
}
dta.470 <- querydb(paste(github_path, "General_Resources/SQL Scripts/Form470s.SQL", sep=""))
bens <- querydb(paste(github_path, "General_Resources/SQL Scripts/Entity_Bens.SQL", sep=""))
dd.2016 <- querydb(paste(github_path, "General_Resources/SQL Scripts/2016_deluxe_districts_crusher_materialized_all.SQL", sep=""))
dd.2016 <- correct.dataset(dd.2016, sots.flag = 0, services.flag = 0)
## disconnect from database
dbDisconnect(con)
## format the column names (take out capitalization and spaces)
names(dta.470) <- tolower(names(dta.470))
names(dta.470) <- gsub(" ", ".", names(dta.470))
## rename column "function"
names(dta.470)[names(dta.470) == 'function'] <- 'function1'
## rename column "470.number"
names(dta.470)[names(dta.470) == '470.number'] <- 'X470.number'
## convert capacity to numeric
dta.470$maximum.capacity.reported <- dta.470$maximum.capacity
dta.470$maximum.capacity <- suppressWarnings(ifelse(grepl('Mbps', dta.470$maximum.capacity), as.numeric(gsub('Mbps', '', dta.470$maximum.capacity)),
as.numeric(gsub('Gbps', '', dta.470$maximum.capacity))*1000))
## merge in BENs to DD
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## merge in number of students and number of campuses
dta.470 <- merge(dta.470, dd.2016[,c('ben', 'esh_id', 'num_students', 'num_campuses')], by='ben', all.x=T)
## the possible missing BENS (NA number of students),
## could be things we don't care about (ie Libraries)
## OR could be one-school districts that used their school BEN instead of their district BEN
## for now, take them out
dta.470.na.students <- dta.470[which(is.na(dta.470$num_students)),]
dta.470 <- dta.470[which(!is.na(dta.470$num_students)),]
## Clearing memory
rm(list=ls())
## running locally or on the server:
local <- 1
## set the current directory as the working directory
if (local == 1){
setwd("~/Documents/ESH-Code/ficher/Projects/form_470/code/")
} else{
wd <- setwd(".")
setwd(wd)
}
## load packages (if not already in the environment)
packages.to.install <- c("DBI", "rJava","RJDBC", "dotenv")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(rJava)
library(RJDBC)
library(DBI)
library(dotenv)
options(java.parameters = "-Xmx4g" )
## retrieve date (in order to accurately timestamp files)
date <- Sys.time()
date <- gsub("PST", "", date)
date <- gsub(" ", "_", date)
date <- gsub(":", ".", date)
## source function
source("source_env.R")
source_env("~/.env")
github_path
url
paste(github_path, "General_Resources/R_database_access/postgresql-9.4.1212.jar", sep="")
pgsql <- JDBC("org.postgresql.Driver", paste(github_path, "General_Resources/R_database_access/postgresql-9.4.1212.jar", sep=""), "`")
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("gridExtra", "ggplot2")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(ggplot2)
library(gridExtra) ## for grid.arrange function
##**************************************************************************************************************************************************
## read in data
dd.2016 <- read.csv("data/raw/deluxe_districts_2016.csv", as.is=T, header=T, stringsAsFactors=F)
dta.470 <- read.csv("data/raw/form_470.csv", as.is=T, header=T, stringsAsFactors=F)
setwd("~/Google Drive/ESH Main Share/Strategic Analysis Team/2017/Org-Wide Projects/Clustering Mega and Large Districts/")
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("gridExtra", "ggplot2")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(ggplot2)
library(gridExtra) ## for grid.arrange function
dd.2016 <- read.csv("data/raw/deluxe_districts_2016.csv", as.is=T, header=T, stringsAsFactors=F)
dta.470 <- read.csv("data/raw/form_470.csv", as.is=T, header=T, stringsAsFactors=F)
##**************************************************************************************************************************************************
## subset and format data
## 470 data formatting
## format the column names (take out capitalization and spaces)
names(dta.470) <- tolower(names(dta.470))
names(dta.470) <- gsub(" ", ".", names(dta.470))
## rename column "function"
names(dta.470)[names(dta.470) == 'function'] <- 'function1'
## rename column "470.number"
names(dta.470)[names(dta.470) == '470.number'] <- 'X470.number'
## merge in BENs to DD
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## keep only unique bens (since some districts file multiple Form 470s)
dta.470 <- dta.470[which(!duplicated(dta.470$ben)),]
## merge in form 470 info (to determine if they've filed one)
dd.2016 <- merge(dd.2016, dta.470[,c('ben', 'X470.number')], by='ben', all.x=T)
## create an indicator for whether a district has filed a form 470
dd.2016$form_470 <- ifelse(!is.na(dd.2016$X470.number), TRUE, FALSE)
## take out duplicated esh_ids
dd.2016 <- dd.2016[!duplicated(dd.2016$esh_id),]
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("gridExtra", "ggplot2")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(ggplot2)
library(gridExtra) ## for grid.arrange function
##**************************************************************************************************************************************************
## read in data
dd.2016 <- read.csv("data/raw/deluxe_districts_2016.csv", as.is=T, header=T, stringsAsFactors=F)
dta.470 <- read.csv("data/raw/form_470.csv", as.is=T, header=T, stringsAsFactors=F)
bens <- read.csv("data/raw/bens.csv", as.is=T, header=T, stringsAsFactors=F)
## 470 data formatting
## format the column names (take out capitalization and spaces)
names(dta.470) <- tolower(names(dta.470))
names(dta.470) <- gsub(" ", ".", names(dta.470))
## rename column "function"
names(dta.470)[names(dta.470) == 'function'] <- 'function1'
## rename column "470.number"
names(dta.470)[names(dta.470) == '470.number'] <- 'X470.number'
## merge in BENs to DD
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## keep only unique bens (since some districts file multiple Form 470s)
dta.470 <- dta.470[which(!duplicated(dta.470$ben)),]
## merge in form 470 info (to determine if they've filed one)
dd.2016 <- merge(dd.2016, dta.470[,c('ben', 'X470.number')], by='ben', all.x=T)
## create an indicator for whether a district has filed a form 470
dd.2016$form_470 <- ifelse(!is.na(dd.2016$X470.number), TRUE, FALSE)
## take out duplicated esh_ids
dd.2016 <- dd.2016[!duplicated(dd.2016$esh_id),]
names(dta.470)
## =========================================
##
## QUERY DATA FROM THE DB
##
## =========================================
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("gridExtra", "ggplot2")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(ggplot2)
library(gridExtra) ## for grid.arrange function
##**************************************************************************************************************************************************
## read in data
dd.2016 <- read.csv("data/raw/deluxe_districts_2016.csv", as.is=T, header=T, stringsAsFactors=F)
dta.470 <- read.csv("data/raw/form_470.csv", as.is=T, header=T, stringsAsFactors=F)
bens <- read.csv("data/raw/bens.csv", as.is=T, header=T, stringsAsFactors=F)
##**************************************************************************************************************************************************
## subset and format data
## 470 data formatting
## format the column names (take out capitalization and spaces)
names(dta.470) <- tolower(names(dta.470))
names(dta.470) <- gsub(" ", ".", names(dta.470))
## rename column "function"
names(dta.470)[names(dta.470) == 'function'] <- 'function1'
names(dta.470)
## merge in BENs to DD
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## keep only unique bens (since some districts file multiple Form 470s)
dta.470 <- dta.470[which(!duplicated(dta.470$ben)),]
## merge in form 470 info (to determine if they've filed one)
dd.2016 <- merge(dd.2016, dta.470[,c('ben', 'x470.number')], by='ben', all.x=T)
## create an indicator for whether a district has filed a form 470
dd.2016$form_470 <- ifelse(!is.na(dd.2016$x470.number), TRUE, FALSE)
## take out duplicated esh_ids
dd.2016 <- dd.2016[!duplicated(dd.2016$esh_id),]
## select mega and large districts
dta <- dd.2016
dta.mega.large <- dd.2016[which(dd.2016$district_size %in% c('Large', 'Mega')),]
pdf("figures/visualize_raw.pdf", height=4, width=10)
plot1 <- ggplot(dta, aes(num_students, num_schools, color=district_size)) + geom_point()
plot2 <- ggplot(dta, aes(num_students, num_schools, color=district_size)) + xlim(0,225000) + ylim(0,400) + geom_point()
grid.arrange(plot1, plot2, ncol=2)
dev.off()
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("gridExtra", "ggplot2", "data.table")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(ggplot2)
library(gridExtra) ## for grid.arrange function
library(data.table) ## for fread function
dd.2016 <- fread("data/raw/deluxe_districts_2016.csv", as.is=T, header=T, stringsAsFactors=F)
dd.2016 <- fread("data/raw/deluxe_districts_2016.csv", stringsAsFactors=F)
View(dd.2016)
dta.470 <- fread("data/raw/form_470.csv", stringsAsFactors=F)
View(dta.470)
warnings()
dta.470 <- fread("data/raw/form_470.csv", stringsAsFactors=F, verbose=T)
dta.470 <- read.csv("data/raw/form_470.csv", as.is=T, header=T, stringsAsFactors=F)
bens <- fread("data/raw/bens.csv", stringsAsFactors=F)
View(bens)
## 470 data formatting
## format the column names (take out capitalization and spaces)
names(dta.470) <- tolower(names(dta.470))
names(dta.470) <- gsub(" ", ".", names(dta.470))
## rename column "function"
names(dta.470)[names(dta.470) == 'function'] <- 'function1'
## merge in BENs to DD
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## keep only unique bens (since some districts file multiple Form 470s)
dta.470 <- dta.470[which(!duplicated(dta.470$ben)),]
## merge in form 470 info (to determine if they've filed one)
dd.2016 <- merge(dd.2016, dta.470[,c('ben', 'x470.number')], by='ben', all.x=T)
## create an indicator for whether a district has filed a form 470
dd.2016$form_470 <- ifelse(!is.na(dd.2016$x470.number), TRUE, FALSE)
## take out duplicated esh_ids
dd.2016 <- dd.2016[!duplicated(dd.2016$esh_id),]
## select mega and large districts
dta <- dd.2016
dta.mega.large <- dd.2016[which(dd.2016$district_size %in% c('Large', 'Mega')),]
names(dd.2016)
names(bens)
dd.2016 <- merge(dd.2016, bens, by.x='esh_id', by.y='entity_id', all.x=T)
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("fpc", "cluster")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(fpc) ## for pamk function
library(cluster) ## for pam and clusGap function
##**************************************************************************************************************************************************
## read in data
dta <- read.csv("data/interim/all_districts.csv", as.is=T, header=T, stringsAsFactors=F)
dta.mega.large <- read.csv("data/interim/mega_large_districts.csv", as.is=T, header=T, stringsAsFactors=F)
## K-Means:
## K-means clustering can handle larger datasets than hierarchical cluster approaches.
## Additionally, observations are not permanently committed to a cluster.
## They are moved when doing so improves the overall solution.
## However, the use of means implies that all variables must be continuous and the approach can be severely affected by outliers.
## They also perform poorly in the presence of non-convex (e.g., U-shaped) clusters.
set.seed(455)
dtaCluster <- kmeans(dta[,c('num_students', 'num_schools')], 20, nstart=20)
dta$cluster_id <- as.factor(dtaCluster$cluster)
dtaCluster <- kmeans(dta.mega.large[,c('num_students', 'num_schools')], 8, nstart=20)
dta.mega.large$cluster_id_mega_large <- as.factor(dtaCluster$cluster)
warnings()
## merge in both indicators
dta <- merge(dta, dta.mega.large[,c('esh_id', 'cluster_id_mega_large')], by='esh_id', all.x=T)
## aggregate cluster means to reorder
dta.agg <- aggregate(dta[,c('num_students', 'num_schools')], by=list(dta$cluster_id), FUN=mean, na.rm=T)
dta.mega.large.agg <- aggregate(dta.mega.large[,c('num_students', 'num_schools')], by=list(dta.mega.large$cluster_id_mega_large), FUN=mean, na.rm=T)
## order clusters based on decreasing mean
dta.agg <- dta.agg[order(dta.agg$num_students, decreasing=T),]
## reassign cluster ids
dta.agg$new_cluster_id <- 1:nrow(dta.agg)
## merge in new cluster ids
dta <- merge(dta, dta.agg[,c('Group.1', 'new_cluster_id')], by.x='cluster_id', by.y='Group.1', all.x=T)
dta$cluster_id <- as.factor(dta$new_cluster_id)
## order clusters based on decreasing mean
dta.mega.large.agg <- dta.mega.large.agg[order(dta.mega.large.agg$num_students, decreasing=T),]
## reassign cluster ids
dta.mega.large.agg$new_cluster_id_mega_large <- 1:nrow(dta.mega.large.agg)
## merge in new cluster ids
dta <- merge(dta, dta.mega.large.agg[,c('Group.1', 'new_cluster_id_mega_large')], by.x='cluster_id', by.y='Group.1', all.x=T)
dta$cluster_id_mega_large <- as.factor(dta$new_cluster_id_mega_large)
## Clearing memory
rm(list=ls())
## load packages (if not already in the environment)
packages.to.install <- c("ggplot2", "gridExtra")
for (i in 1:length(packages.to.install)){
if (!packages.to.install[i] %in% rownames(installed.packages())){
install.packages(packages.to.install[i])
}
}
library(ggplot2)
library(gridExtra) ## for grid.arrange function
##**************************************************************************************************************************************************
## read in data
dta <- read.csv("data/processed/clustered_all_districts.csv", as.is=T, header=T, stringsAsFactors=F)
dta.agg <- read.csv("data/processed/aggregated_means_clusters_all_districts.csv", as.is=T, header=T, stringsAsFactors=F)
dta.mega.large.agg <- read.csv("data/processed/aggregated_means_clusters_mega_and_large_districts.csv", as.is=T, header=T, stringsAsFactors=F)
