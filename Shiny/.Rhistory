library(leaflet)
library(ggvis)
library(shinydashboard)
library(DT) #for datatables
library(shinyjs) #for reset button
library(mapview)
library(leaflet)
library(ggvis)
library(shinydashboard)
library(DT) #for datatables
library(shinyjs) #for reset button
library(mapview)
shiny::runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
runApp('Desktop/ficher/Shiny')
## Create reactive functions for both services received and districts table ##
library(shiny)
library(tidyr)
library(dplyr)
library(ggplot2)
library(scales)
library(grid)
library(maps)
library(ggmap)
library(reshape)
library(leaflet)
library(ggvis)
library(DT)
library(shinydashboard)
library(extrafontdb)
library(extrafont)
library(leaflet)
library(ggvis)
library(shinydashboard)
library(DT) #for datatables
library(shinyjs) #for reset button
library(mapview)
sketch = htmltools::withTags(table(
class = 'display',
thead(
tr(
th(rowspan = 2, 'Species'),
th(colspan = 2, 'Sepal'),
th(colspan = 2, 'Petal')
),
tr(
lapply(rep(c('Length', 'Width'), 2), th)
)
)
))
print(sketch)
lapply(rep(c('Length', 'Width'), 2), th)
tr(
lapply(rep(c('Length', 'Width'), 2), th)
)
sketch = htmltools::withTags(table(
class = 'display',
thead(
tr(
th(rowspan = 2, 'Postal Code'),
th(rowspan = 2, 'Locale'),
th(colspan = 2, 'Clean'),
th(colspan = 2, 'All')
),
tr(
lapply(rep(c('Length', 'Width'), 2), th)
)
)
))
print(sketch)
dd <- read.csv("~/Desktop/delux8.8.csv")
flags <- read.csv("~/Desktop/1000SamplePublicEntityFlags.csv")
View(dd)
length(unique(dd$esh_id))
View(flags)
length(unique(flags$entity_id))
sub <- flags[duplicated(flags$entity_id),]
View(sub)
flags.dup <- flags[flags$entity_id %in% sub$entity_id,]
View(flags)
View(flags.dup)
flags.dup <- flags.dup[order(flags.dup$entity_id),]
View(flags.dup)
names(flags.dup)
names(flags.dup)[names(flags.dup) %in% names(dd)]
dd.sub <- dd[dd$esh_id %in% flags.dup$entity_id,]
View(dd.sub)
dta <- merge(dd.sub, flags.dup, by.x='esh_id', by.y='entity_id', all.y=T)
View(dta)
View(dd.sub)
dd <- read.csv("~/Desktop/delux8.8.csv")
flags <- read.csv("~/Desktop/1000SamplePublicEntityFlags.csv")
sub <- flags[duplicated(flags$entity_id),]
flags.dup <- flags[flags$entity_id %in% sub$entity_id,]
flags.dup <- flags.dup[order(flags.dup$entity_id),]
View(flags.dup)
View(flags.dup)
names(flags.dup)[names(flags.dup) %in% names(dd)]
dd.sub <- dd[dd$esh_id %in% flags.dup$entity_id,]
View(dd.sub)
View(flags.dup)
View(flags)
View(flags.dup)
View(dd.sub)
dta <- merge(dd.sub, flags.dup, by.x='esh_id', by.y='entity_id', all.y=T)
View(dta)
dta <- merge(dd.sub, flags.dup, by.x='esh_id', by.y='entity_id', all.x=T)
View(dta)
dta <- merge(dd.sub, flags.dup, by.x='esh_id', by.y='entity_id', all=T)
View(dta)
dta <- merge(dd, flags, by.x='esh_id', by.y='entity_id', all.y=T)
View(dta)
nrow(flags$entity_id[!flags$entity_id %in% dd$esh_id,])
nrow(flags$entity_id[!flags$entity_id %in% dd$esh_id])
nrow(flags[!flags$entity_id %in% dd$esh_id,])
## =========================================================================
##
## ANALYSIS: Which districts upgraded their infrastructure?
## Aha Card: SAT-1845
## Sprint 08/01/2016 -- AJB
##
## Upgrade defined as
## Increased amount of IA bandwidth purchased
## NOTE: should be change in total bandwidth, not bandwidth per student, since number of students change every year.
## Change connect type from copper to fiber/fiber equivalent for WAN or Internet lines
## Increased amount of WAN bandwidth purchased
##
## add in: breakdown by State, locale, district size, 2015 technology, by purpose, reimbursment rate
## talk to kimberly directly after
##
## =========================================================================
## Clearing memory
rm(list=ls())
setwd("~/Google Drive/PT Analysis/code/")
## loading packages
#install.packages("tigris")
#install.packages("leaflet")
library(rgdal)
library(ggplot2)
library(gpclib)
library(maptools)
library(rgeos)
library(dplyr)
library(tigris)
library(leaflet)
##*********************************************************************************************************
## TOGGLES
## Breakdown Stats:
## 1 - by State
## 2 - by Locale
## 3 - by District Size
## 4 - by 2015 Technology
## 5 - by Purpose
## 6 - by Reimbursment Rate
breakdown <- 1
## if subsetting on state, select postal code abbreviation
## else select 'ALL' if we want all states
if (breakdown == 1){
state <- 'NH'
}
##*********************************************************************************************************
## READ IN FILES
## 2015 ESH Deluxe table (district-level)
deluxe.2015 <- read.csv("../data/2015/deluxe-districts-table-2016-08-01.csv", as.is=T, header=T)
## 2016 ESH District Aggregation table (district-level)
deluxe.agg.2016 <- read.csv("../data/2016/2016_district_aggregation-2016-08-01.csv", as.is=T, header=T)
## 2016 ESH District Metrics table (district-level)
deluxe.met.2016 <- read.csv("../data/2016/2016_districts_metrics-2016-08-02.csv", as.is=T, header=T)
## 2016 ESH District Demographics table (district-level)
deluxe.demo.2016 <- read.csv("../data/2016/2016_districts_demog-2016-08-02.csv", as.is=T, header=T)
## combine 2016 data sets
## merge on esh_id and district_esh_id
## look at overlapping columns
names(deluxe.agg.2016)[names(deluxe.agg.2016) %in% names(deluxe.met.2016)]
deluxe.2016 <- merge(deluxe.agg.2016, deluxe.met.2016[,names(deluxe.met.2016)[!(names(deluxe.met.2016) %in% names(deluxe.agg.2016))]],
by.x='district_esh_id', by.y='esh_id', all=T)
## it appears deluxe.demo.2016 is now a complete subset of deluxe.2016
ncol(deluxe.demo.2016)
length(names(deluxe.2016)[names(deluxe.2016) %in% names(deluxe.demo.2016)])
names(deluxe.2016)[names(deluxe.2016) == "district_esh_id"] <- "esh_id"
## subset to only clean districts
deluxe.2015 <- deluxe.2015[deluxe.2015$exclude_from_analysis == FALSE,]
deluxe.2016 <- deluxe.2016[deluxe.2016$flag_array == "",]
deluxe.2016 <- deluxe.2016[!is.na(deluxe.2016$esh_id),]
## subset both datasets to overlapping clean districts
overlap.2016 <- deluxe.2016$esh_id[deluxe.2016$esh_id %in% deluxe.2015$esh_id]
overlap.2015 <- deluxe.2015$esh_id[deluxe.2015$esh_id %in% deluxe.2016$esh_id]
overlap <- overlap.2016[overlap.2016 %in% overlap.2015]
deluxe.2015 <- deluxe.2015[deluxe.2015$esh_id %in% overlap,]
deluxe.2016 <- deluxe.2016[deluxe.2016$esh_id %in% overlap,]
deluxe.2016 <- deluxe.2016[order(deluxe.2016$esh_id),]
deluxe.2015 <- deluxe.2015[order(deluxe.2015$esh_id),]
## subset to toggles
if (breakdown == 1){
if (state == 'ALL'){
deluxe.2015 <- deluxe.2015
deluxe.2016 <- deluxe.2016
} else{
deluxe.2015 <- deluxe.2015[deluxe.2015$postal_cd == state,]
deluxe.2016 <- deluxe.2016[deluxe.2016$postal_cd == state,]
}
}
## prep data table
dta.table <- data.frame(matrix(NA, nrow=10, ncol=2))
View(dta.table)
dta <- deluxe.2016[,c('esh_id', 'ia_bandwidth')]
dta <- merge(dta, deluxe.2015[,c('esh_id', 'total_ia_bw_mbps', 'postal_cd')], by='esh_id', all=T)
names(dta) <- c('esh_id', 'internet_bandwidth_2016', 'internet_bandwidth_2015', 'postal_cd')
dta$diff <- dta$internet_bandwidth_2016 - dta$internet_bandwidth_2015
## create counter for 10% increase
dta$counter.10.increase <- ifelse(dta$diff > .10*dta$internet_bandwidth_2015, 1, 0)
table(dta$counter.10.increase)
dta$counter <- 1
## where are these increases happening? -- look by state
dta.increase <- dta[dta$counter.10.increase == 1,]
dta.state.agg <- aggregate(dta.increase$counter.10.increase, by=list(dta.increase$postal_cd), FUN=sum)
names(dta.state.agg) <- c('postal_cd', 'percent.increase')
dta.state.total <- aggregate(dta$counter, by=list(dta$postal_cd), FUN=sum)
names(dta.state.total) <- c('postal_cd', 'total')
dta.state.agg <- merge(dta.state.agg, dta.state.total, by='postal_cd', all=T)
dta.state.agg$percent.increase <- dta.state.agg$percent.increase / dta.state.agg$total
## where are these increases happening? -- look by state
dta.increase <- dta[dta$counter.10.increase == 1,]
dta.state.agg <- aggregate(dta.increase$counter.10.increase, by=list(dta.increase$postal_cd), FUN=sum)
names(dta.state.agg) <- c('postal_cd', 'percent.increase')
dta.state.total <- aggregate(dta$counter, by=list(dta$postal_cd), FUN=sum)
names(dta.state.total) <- c('postal_cd', 'total')
dta.state.agg <- merge(dta.state.agg, dta.state.total, by='postal_cd', all=T)
dta.state.agg$percent.increase <- dta.state.agg$percent.increase / dta.state.agg$total
View(dta.state.total)
rownames(dta.table)[1] <- 'Number of districts increased bandwidth by 10%'
View(dta.table)
print(dta.table)
dta.table[1,1] <- 'Number of districts increased bandwidth by 10%'
View(dta.table)
dta.table <- data.frame(matrix(NA, nrow=10, ncol=1))
rownames(dta.table)[1] <- 'Number of districts increased bandwidth by 10%'
View(dta.table)
names(dta.table) <- ''
View(dta.table)
## =========================================================================
##
## ANALYSIS: Which districts upgraded their infrastructure?
## Aha Card: SAT-1845
## Sprint 08/01/2016 -- AJB
##
## Upgrade defined as
## Increased amount of IA bandwidth purchased
## NOTE: should be change in total bandwidth, not bandwidth per student, since number of students change every year.
## Change connect type from copper to fiber/fiber equivalent for WAN or Internet lines
## Increased amount of WAN bandwidth purchased
##
## add in: breakdown by State, locale, district size, 2015 technology, by purpose, reimbursment rate
## talk to kimberly directly after
##
## =========================================================================
## Clearing memory
rm(list=ls())
setwd("~/Google Drive/PT Analysis/code/")
## loading packages
#install.packages("tigris")
#install.packages("leaflet")
library(rgdal)
library(ggplot2)
library(gpclib)
library(maptools)
library(rgeos)
library(dplyr)
library(tigris)
library(leaflet)
##*********************************************************************************************************
## TOGGLES
## Breakdown Stats:
## 1 - by State
## 2 - by Locale
## 3 - by District Size
## 4 - by 2015 Technology
## 5 - by Purpose
## 6 - by Reimbursment Rate
#breakdown <- 1
## if subsetting on state, select postal code abbreviation
## else select 'ALL' if we want all states
#if (breakdown == 1){
state <- 'NH'
#}
##*********************************************************************************************************
## READ IN FILES
## 2015 ESH Deluxe table (district-level)
deluxe.2015 <- read.csv("../data/2015/deluxe-districts-table-2016-08-01.csv", as.is=T, header=T)
## 2016 ESH District Aggregation table (district-level)
deluxe.agg.2016 <- read.csv("../data/2016/2016_district_aggregation-2016-08-01.csv", as.is=T, header=T)
## 2016 ESH District Metrics table (district-level)
deluxe.met.2016 <- read.csv("../data/2016/2016_districts_metrics-2016-08-02.csv", as.is=T, header=T)
## 2016 ESH District Demographics table (district-level)
deluxe.demo.2016 <- read.csv("../data/2016/2016_districts_demog-2016-08-02.csv", as.is=T, header=T)
## state shapefiles
#state.shp <- readOGR(dsn="../data/External/Shapefiles/US State 5m/", layer = "cb_2015_us_state_5m")
# Retain only plotting data (as opposed to layer data that is stored in state.shp@data)
#state.shp.df = fortify(state.shp, region="STUSPS")
##*********************************************************************************************************
## combine 2016 data sets
## merge on esh_id and district_esh_id
## look at overlapping columns
names(deluxe.agg.2016)[names(deluxe.agg.2016) %in% names(deluxe.met.2016)]
deluxe.2016 <- merge(deluxe.agg.2016, deluxe.met.2016[,names(deluxe.met.2016)[!(names(deluxe.met.2016) %in% names(deluxe.agg.2016))]],
by.x='district_esh_id', by.y='esh_id', all=T)
## it appears deluxe.demo.2016 is now a complete subset of deluxe.2016
ncol(deluxe.demo.2016)
length(names(deluxe.2016)[names(deluxe.2016) %in% names(deluxe.demo.2016)])
names(deluxe.2016)[names(deluxe.2016) == "district_esh_id"] <- "esh_id"
## subset to only clean districts
deluxe.2015 <- deluxe.2015[deluxe.2015$exclude_from_analysis == FALSE,]
deluxe.2016 <- deluxe.2016[deluxe.2016$flag_array == "",]
deluxe.2016 <- deluxe.2016[!is.na(deluxe.2016$esh_id),]
## subset both datasets to overlapping clean districts
overlap.2016 <- deluxe.2016$esh_id[deluxe.2016$esh_id %in% deluxe.2015$esh_id]
overlap.2015 <- deluxe.2015$esh_id[deluxe.2015$esh_id %in% deluxe.2016$esh_id]
overlap <- overlap.2016[overlap.2016 %in% overlap.2015]
deluxe.2015 <- deluxe.2015[deluxe.2015$esh_id %in% overlap,]
deluxe.2016 <- deluxe.2016[deluxe.2016$esh_id %in% overlap,]
deluxe.2016 <- deluxe.2016[order(deluxe.2016$esh_id),]
deluxe.2015 <- deluxe.2015[order(deluxe.2015$esh_id),]
## subset to toggles
#if (breakdown == 1){
if (state == 'ALL'){
deluxe.2015 <- deluxe.2015
deluxe.2016 <- deluxe.2016
} else{
deluxe.2015 <- deluxe.2015[deluxe.2015$postal_cd == state,]
deluxe.2016 <- deluxe.2016[deluxe.2016$postal_cd == state,]
}
#}
## prep data table
dta.table <- data.frame(matrix(NA, nrow=10, ncol=1))
names(dta.table) <- ''
dta <- deluxe.2016[,c('esh_id', 'ia_bandwidth')]
dta <- merge(dta, deluxe.2015[,c('esh_id', 'total_ia_bw_mbps', 'postal_cd')], by='esh_id', all=T)
names(dta) <- c('esh_id', 'internet_bandwidth_2016', 'internet_bandwidth_2015', 'postal_cd')
dta$diff <- dta$internet_bandwidth_2016 - dta$internet_bandwidth_2015
## create counter for 10% increase
dta$counter.10.increase <- ifelse(dta$diff > .10*dta$internet_bandwidth_2015, 1, 0)
table(dta$counter.10.increase)
dta$counter <- 1
## where are these increases happening? -- look by state
dta.increase <- dta[dta$counter.10.increase == 1,]
dta.state.agg <- aggregate(dta.increase$counter.10.increase, by=list(dta.increase$postal_cd), FUN=sum)
names(dta.state.agg) <- c('postal_cd', 'percent.increase')
dta.state.total <- aggregate(dta$counter, by=list(dta$postal_cd), FUN=sum)
names(dta.state.total) <- c('postal_cd', 'total')
dta.state.agg <- merge(dta.state.agg, dta.state.total, by='postal_cd', all=T)
dta.state.agg$percent.increase <- dta.state.agg$percent.increase / dta.state.agg$total
## fill state polygons based on gradient
#state_bound <- states(unique(dta.state.agg$postal_cd), "lower", cb =TRUE, resolution="5m")
#pal <- colorNumeric("Blues", leg_bound@data$unscalable_schools)
#poly_colors <- district_subset() %>% group_by(senatorial_id) %>% summarise(unscalable_schools = sum(schools_may_need_upgrades, schools_need_upgrades))
#leg_bound@data <- left_join(leg_bound@data, poly_colors, by = c("GEOID" = "senatorial_id"))
#leaflet(leg_bound) %>% addPolygons(fillColor = ~pal(unscalable_schools), color = "#000000", weight = 1,  fillOpacity = 0.3, popup = ~leg_info)
#ggplot() + geom_polygon(data = state.shp.df,
#               aes(x = long, y = lat, group = group),
#               color = "black", size = 0.25) + coord_map()
## list of districts that upgraded
#districts.upgraded <- deluxe.2015$name[deluxe.2015$esh_id %in% dta$esh_id[dta$counter.10.increase == 1]]
rownames(dta.table)[1] <- 'Number of districts increased bandwidth by 10%'
dta.table[1] <- dta.state.agg
View(dta.state.agg)
dta.table[1] <- dta.state.agg$total[1]
View(dta.table)
dta.table[1,1] <- dta.state.agg$total[1]
View(dta.table)
## =========================================================================
##
## ANALYSIS: Which districts upgraded their infrastructure?
## Aha Card: SAT-1845
## Sprint 08/01/2016 -- AJB
##
## Upgrade defined as
## Increased amount of IA bandwidth purchased
## NOTE: should be change in total bandwidth, not bandwidth per student, since number of students change every year.
## Change connect type from copper to fiber/fiber equivalent for WAN or Internet lines
## Increased amount of WAN bandwidth purchased
##
## add in: breakdown by State, locale, district size, 2015 technology, by purpose, reimbursment rate
## talk to kimberly directly after
##
## =========================================================================
## Clearing memory
rm(list=ls())
setwd("~/Google Drive/PT Analysis/code/")
## loading packages
#install.packages("tigris")
#install.packages("leaflet")
library(rgdal)
library(ggplot2)
library(gpclib)
library(maptools)
library(rgeos)
library(dplyr)
library(tigris)
library(leaflet)
##*********************************************************************************************************
## TOGGLES
## Breakdown Stats:
## 1 - by State
## 2 - by Locale
## 3 - by District Size
## 4 - by 2015 Technology
## 5 - by Purpose
## 6 - by Reimbursment Rate
#breakdown <- 1
## if subsetting on state, select postal code abbreviation
## else select 'ALL' if we want all states
#if (breakdown == 1){
state <- 'NH'
#}
##*********************************************************************************************************
## READ IN FILES
## 2015 ESH Deluxe table (district-level)
deluxe.2015 <- read.csv("../data/2015/deluxe-districts-table-2016-08-01.csv", as.is=T, header=T)
## 2016 ESH District Aggregation table (district-level)
deluxe.agg.2016 <- read.csv("../data/2016/2016_district_aggregation-2016-08-01.csv", as.is=T, header=T)
## 2016 ESH District Metrics table (district-level)
deluxe.met.2016 <- read.csv("../data/2016/2016_districts_metrics-2016-08-02.csv", as.is=T, header=T)
## 2016 ESH District Demographics table (district-level)
deluxe.demo.2016 <- read.csv("../data/2016/2016_districts_demog-2016-08-02.csv", as.is=T, header=T)
## state shapefiles
#state.shp <- readOGR(dsn="../data/External/Shapefiles/US State 5m/", layer = "cb_2015_us_state_5m")
# Retain only plotting data (as opposed to layer data that is stored in state.shp@data)
#state.shp.df = fortify(state.shp, region="STUSPS")
##*********************************************************************************************************
## combine 2016 data sets
## merge on esh_id and district_esh_id
## look at overlapping columns
names(deluxe.agg.2016)[names(deluxe.agg.2016) %in% names(deluxe.met.2016)]
deluxe.2016 <- merge(deluxe.agg.2016, deluxe.met.2016[,names(deluxe.met.2016)[!(names(deluxe.met.2016) %in% names(deluxe.agg.2016))]],
by.x='district_esh_id', by.y='esh_id', all=T)
## it appears deluxe.demo.2016 is now a complete subset of deluxe.2016
ncol(deluxe.demo.2016)
length(names(deluxe.2016)[names(deluxe.2016) %in% names(deluxe.demo.2016)])
names(deluxe.2016)[names(deluxe.2016) == "district_esh_id"] <- "esh_id"
## subset to only clean districts
deluxe.2015 <- deluxe.2015[deluxe.2015$exclude_from_analysis == FALSE,]
deluxe.2016 <- deluxe.2016[deluxe.2016$flag_array == "",]
deluxe.2016 <- deluxe.2016[!is.na(deluxe.2016$esh_id),]
## subset both datasets to overlapping clean districts
overlap.2016 <- deluxe.2016$esh_id[deluxe.2016$esh_id %in% deluxe.2015$esh_id]
overlap.2015 <- deluxe.2015$esh_id[deluxe.2015$esh_id %in% deluxe.2016$esh_id]
overlap <- overlap.2016[overlap.2016 %in% overlap.2015]
deluxe.2015 <- deluxe.2015[deluxe.2015$esh_id %in% overlap,]
deluxe.2016 <- deluxe.2016[deluxe.2016$esh_id %in% overlap,]
deluxe.2016 <- deluxe.2016[order(deluxe.2016$esh_id),]
deluxe.2015 <- deluxe.2015[order(deluxe.2015$esh_id),]
## subset to toggles
#if (breakdown == 1){
if (state == 'ALL'){
deluxe.2015 <- deluxe.2015
deluxe.2016 <- deluxe.2016
} else{
deluxe.2015 <- deluxe.2015[deluxe.2015$postal_cd == state,]
deluxe.2016 <- deluxe.2016[deluxe.2016$postal_cd == state,]
}
#}
## prep data table
dta.table <- data.frame(matrix(NA, nrow=10, ncol=1))
names(dta.table) <- ''
##==================================================================================
## 1) Increased amount of bandwidth and associated impact on goals meeting status.
## AND
## What is the average bandwidth change for districts in every state?
## AND
## Which specific districts upgraded? Provide a list.
##==================================================================================
dta <- deluxe.2016[,c('esh_id', 'ia_bandwidth')]
dta <- merge(dta, deluxe.2015[,c('esh_id', 'total_ia_bw_mbps', 'postal_cd')], by='esh_id', all=T)
names(dta) <- c('esh_id', 'internet_bandwidth_2016', 'internet_bandwidth_2015', 'postal_cd')
dta$diff <- dta$internet_bandwidth_2016 - dta$internet_bandwidth_2015
## create counter for 10% increase
dta$counter.10.increase <- ifelse(dta$diff > .10*dta$internet_bandwidth_2015, 1, 0)
table(dta$counter.10.increase)
dta$counter <- 1
## where are these increases happening? -- look by state
dta.increase <- dta[dta$counter.10.increase == 1,]
dta.state.agg <- aggregate(dta.increase$counter.10.increase, by=list(dta.increase$postal_cd), FUN=sum)
names(dta.state.agg) <- c('postal_cd', 'percent.increase')
dta.state.total <- aggregate(dta$counter, by=list(dta$postal_cd), FUN=sum)
names(dta.state.total) <- c('postal_cd', 'total')
dta.state.agg <- merge(dta.state.agg, dta.state.total, by='postal_cd', all=T)
dta.state.agg$percent.increase <- dta.state.agg$percent.increase / dta.state.agg$total
## fill state polygons based on gradient
#state_bound <- states(unique(dta.state.agg$postal_cd), "lower", cb =TRUE, resolution="5m")
#pal <- colorNumeric("Blues", leg_bound@data$unscalable_schools)
#poly_colors <- district_subset() %>% group_by(senatorial_id) %>% summarise(unscalable_schools = sum(schools_may_need_upgrades, schools_need_upgrades))
#leg_bound@data <- left_join(leg_bound@data, poly_colors, by = c("GEOID" = "senatorial_id"))
#leaflet(leg_bound) %>% addPolygons(fillColor = ~pal(unscalable_schools), color = "#000000", weight = 1,  fillOpacity = 0.3, popup = ~leg_info)
#ggplot() + geom_polygon(data = state.shp.df,
#               aes(x = long, y = lat, group = group),
#               color = "black", size = 0.25) + coord_map()
## list of districts that upgraded
#districts.upgraded <- deluxe.2015$name[deluxe.2015$esh_id %in% dta$esh_id[dta$counter.10.increase == 1]]
rownames(dta.table)[1] <- 'Number of districts increased bandwidth by 10%'
dta.table[1,1] <- dta.state.agg$total[1]
View(dta.table)
View(dta)
dta.table[2,1] <- mean(dta$diff)
View(dta.table)
shiny::runApp('~/Desktop/ficher/Shiny')
runApp('~/Desktop/ficher/Shiny')
## Clearing memory
rm(list=ls())
runApp('~/Desktop/ficher/Shiny')
